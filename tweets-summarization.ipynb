{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_news_tweets = pd.read_pickle('en-emd-paraphrase-distilroberta-base-v2.pkl')\n",
    "# df_news_tweets = pd.read_pickle('en-emd-paraphrase-mpnet-base-v2.pkl')\n",
    "# df_news_tweets = pd.read_pickle('en-emd-paraphrase-MiniLM-L6-v2.pkl')\n",
    "# df_news_tweets = pd.read_pickle('en-emd-nli-mpnet-base-v2.pkl')\n",
    "# df_news_tweets = pd.read_pickle('en-emd-nli-roberta-base-v2.pkl')\n",
    "# df_news_tweets = pd.read_pickle('en-emd-digitalepidemiologylab-covid-twitter-bert-v2.pkl')\n",
    "# df_news_tweets = pd.read_pickle('en-emd-cardiffnlp-twitter-roberta-base.pkl')\n",
    "df_news_tweets = pd.read_pickle('en-emd-facebook-laser.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_labels = pd.read_pickle('Leiden_CosTh85.pkl')\n",
    "# df_cluster_labels = pd.read_pickle('Agglomerative_CosTh85.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataframe of each cluster\n",
    "dict_com = {}\n",
    "for i in range(len(set(df_cluster_labels['com_label']))):\n",
    "    dict_com[i] = df_cluster_labels.loc[df_cluster_labels['com_label'] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated and near-duplicated tweets by seting a larger similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglom_cluster(corpus, embeddings, threshold):\n",
    "    cos_mat = util.cos_sim(embeddings, embeddings)\n",
    "    cos_copy = cos_mat.detach().cpu().numpy()\n",
    "    clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=threshold, affinity='precomputed', linkage='average')\n",
    "    clustering_model.fit(1-cos_copy)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    clustered_sentences = {}\n",
    "    clustered_ids = {}\n",
    "    for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "        if cluster_id not in clustered_sentences:\n",
    "            clustered_sentences[cluster_id] = []\n",
    "            clustered_ids[cluster_id] = []\n",
    "\n",
    "        clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "        clustered_ids[cluster_id].append(sentence_id)\n",
    "    communities = sorted(clustered_ids.items(), key= lambda x: len(x[1]), reverse=True) \n",
    "    \n",
    "    return communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contatenate tweets text and news summaries in each cluster\n",
    "total_tweets_summaries = []\n",
    "total_news_ids = []\n",
    "total_news_summaries = []\n",
    "total_com_ids = []\n",
    "\n",
    "for com in range(len(dict_com)):\n",
    "#     print(com)\n",
    "    total_com_ids.append(com)\n",
    "    news_id_set = set(dict_com[com]['news_id'])\n",
    "    news_ids = ','.join(news_id_set)\n",
    "    total_news_ids.append(news_ids)\n",
    "    \n",
    "    news_set = set(dict_com[com]['news_summary'])\n",
    "    news_summaries = '\\n'.join(news_set)\n",
    "    total_news_summaries.append(news_summaries)\n",
    "    \n",
    "    new_com = dict_com[com]\n",
    "    \n",
    "    sub_communities = agglom_cluster(new_com['clean_text'].tolist(), new_com['tweet_embeddings'].tolist(),threshold = 0.1)\n",
    "    df_coms = {}\n",
    "    i = 0\n",
    "    for sub in sub_communities:\n",
    "        df_coms[i]= new_com.loc[new_com.index[sub[1]]]\n",
    "        i += 1\n",
    "    \n",
    "    summaries = []\n",
    "    for i in range(len(sub_communities)):\n",
    "        summaries.append(df_coms[i]['clean_text'].tolist()[0])\n",
    "        \n",
    "    summaries = '\\n'.join(summaries)\n",
    "        \n",
    "    total_tweets_summaries.append(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive summarization\n",
    "Models: Bart, T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', model = 'facebook/bart-large-cnn', device =0)\n",
    "# summarizer = pipeline('summarization', model = 't5-large', device =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 140, but you input_length is only 99. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 103. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 129. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 127. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 59. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 82. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 99. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 112. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 140, but you input_length is only 136. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "total_summaries = []\n",
    "total = []\n",
    "for i in range(0, len(dict_com)-15, 15):\n",
    "    sumry = summarizer(total_tweets_summaries[i:i+15], truncation = True, min_length = 10, max_length = 140)\n",
    "    total+= ([s['summary_text'] for s in sumry])\n",
    "sumry = summarizer(total_tweets_summaries[i+15:], truncation = True, min_length = 10,  max_length = 140)\n",
    "total += ([s['summary_text'] for s in sumry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sumry = ['\\n'.join(sent_tokenize(para)) for para in total] # Join sentences with '\\n' to be the same format as news summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_summary = {'news_ids': total_news_ids, 'news_summaries': total_news_summaries, 'tweets_summaries': total_sumry, 'tweets':total_tweets_summaries} \n",
    "df_summary = pd.DataFrame(dict_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_pickle('bart_leiden.pkl')\n",
    "# df_summary.to_pickle('t5_leiden.pkl')\n",
    "# df_summary.to_pickle('bart_agglomerative.pkl')\n",
    "# df_summary.to_pickle('t5_agglomerative.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ROUGE score for summarization evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge = load_metric('rouge')\n",
    "bertscore = load_metric('bertscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_results = rouge.compute(predictions = df_summary['tweets_summaries'].tolist(), references = df_summary['news_summaries'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.43368526759846837, recall=0.6325468508516967, fmeasure=0.47947813401556355), mid=Score(precision=0.4530424028216105, recall=0.6558864013238046, fmeasure=0.49935404988195053), high=Score(precision=0.47315352025898383, recall=0.6803422645131049, fmeasure=0.520343374349764)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.3181877067227712, recall=0.46972141044030025, fmeasure=0.35453257492847434), mid=Score(precision=0.3422487225221194, recall=0.49900197543518265, fmeasure=0.380818591030837), high=Score(precision=0.366161424297369, recall=0.5276762253629017, fmeasure=0.40542384776988283)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.39932435241284453, recall=0.5819279741294705, fmeasure=0.44282582296705614), mid=Score(precision=0.4210870556787015, recall=0.6065152622973269, fmeasure=0.4646634433377768), high=Score(precision=0.443205122204518, recall=0.6304697352802568, fmeasure=0.48590849803516917)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.40847677457909143, recall=0.5985367402077231, fmeasure=0.45474219679083255), mid=Score(precision=0.43058725195571124, recall=0.6240295263714104, fmeasure=0.4763255079075535), high=Score(precision=0.4517378945597439, recall=0.647487003575986, fmeasure=0.4971589266507636))}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = bertscore.compute(predictions = df_summary['tweets_summaries'].tolist(), references = df_summary['news_summaries'].tolist(), lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9076200063346971"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bert_results['f1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
